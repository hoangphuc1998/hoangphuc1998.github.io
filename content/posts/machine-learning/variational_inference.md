---
title: "Variational Inference"
# weight: 1
# aliases: ["/first"]
tags: ["statistics", "generative-models"]
date: "2022-06-26"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Desc Text."
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: true
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
categories: ["machine-learning"]
# url: /machine-learning/variational_inference
editPost:
    URL: "https://github.com/hoangphuc1998/hoangphuc1998.github.io/content"
    Text: "Suggest Changes" # edit text
    appendFilePath: true # to append file path to Edit link
---

# 1. What is Bayesian inference?
(In this post, I will only talk about inference in Bayesian network, but the idea remain the same when applying to general Probabilistic Graphical Models)

First of all, we need to know what is an Bayesian network.
- Two questions:
    - Marginal inference
    - Maximum a posteriori
- Example:
- Inference in machine learning world is a special case of inference in graphical probabilistic models.
# 2. Why inference is hard?
- Multidimensionality
- Example
- Sampling techniques
- Advantages and disadvantages of sampling techniques
# 3. Inference as Optimization problem

# Reference
- CS228: Probabilistic Graphical Models of Stanford University ![https://ermongroup.github.io/cs228-notes](https://ermongroup.github.io/cs228-notes)