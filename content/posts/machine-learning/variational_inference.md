---
title: "Variational Inference"
# weight: 1
# aliases: ["/first"]
tags: ["statistics", "generative-models"]
date: "2022-06-26"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "Desc Text."
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: true
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
categories: ["machine-learning"]
# url: /machine-learning/variational_inference
editPost:
    URL: "https://github.com/hoangphuc1998/hoangphuc1998.github.io/content"
    Text: "Suggest Changes" # edit text
    appendFilePath: true # to append file path to Edit link
---

# 1. What is Bayesian inference?
(In this post, I will only talk about inference in Bayesian network, but the idea remain the same when applying to general Probabilistic Graphical Models)

First of all, it is good to know what is a Bayesian network. A Bayesian network is a **directed acyclic graph** (DAG) that represents the joint probability of a set of random variables. The vertices of the graph 
- Two questions:
    - Marginal inference
    - Maximum a posteriori
- Example:
# 2. Why inference is hard?
- Multidimensionality
- Example
- Sampling techniques
- Advantages and disadvantages of sampling techniques
# 3. Inference as Optimization problem

# Reference
- CS228: Probabilistic Graphical Models of Stanford University ![https://ermongroup.github.io/cs228-notes](https://ermongroup.github.io/cs228-notes)